---
title: Full Autonomy
description: "Full autonomy architecture and introduction"
---

OM1 supports Full Autonomy Mode on both Nvidia AGX and Thor platforms, enabling advanced perception, mapping, navigation, and interaction capabilities with minimal human input.

   - AGX: Supported with a limited feature set.

   - Thor: Provides comprehensive support for full autonomy.

On the Nvidia Thor platform, OM1 includes advanced machine learning capabilities such as:

   - Face recognition

   - Anonymization

Thor is fully supported and optimized for ML-driven workloads, making it the recommended platform for deployments requiring advanced autonomy and perception features.

The Full Autonomy stack is built from modular, containerized services that communicate through well-defined interfaces. The diagram below shows how these components interact within the system.

![Architecture Diagram](../assets/full-autonomy-assets/full_autonomy_architecture.png)

## Core Components
OM1’s full autonomy is powered by the following tightly integrated services, each running in its own container and communicating in real time:

### 1. OM1 (`om1`)
The central intelligence of the system, responsible for:
- High-level decision making and behavior planning
- Natural language understanding and generation
- Task planning and execution monitoring
- Integration with external AI services

### 2. Unitree SDK (`unitree_sdk`)
The robotics middleware that provides:
- Sensor data acquisition and processing
- SLAM (Simultaneous Localization and Mapping)
- Navigation and motion planning
- Low-level robot control
- Auto charging

### 3. OM1 Avatar (`om1-avatar`)
Frontend interface components:
- React-based user interface
- Real-time avatar rendering
- System status visualization
- User interaction layer
- Runs on the robot's BrainPack screen

### 4. Video Processor (`om1-video-processor`)
Media handling subsystem:
- Camera feed processing
- Face detection and recognition
- Video and audio streaming to RTSP server
- Face detection and anonymisation

We currently provide full autonomy for Unitree G1 and Go2 through the BrainPack.

## System Architecture
1. **Sensor Data Collection** - The unitree_sdk gathers data from sensors such as LiDAR and cameras, publishing ROS2 topics for localization and mapping.

2. **Autonomous Decision-Making** - The om1 agent receives processed sensor data, user and system prompts, system governance and issues commands for navigation, exploration, and interaction, enabling the robot to operate independently.

3. **Continuous Monitoring** - The watchdog service within the ROS2 SDK monitors sensor and topic health, automatically restarting components if issues are detected to ensure reliability.

4. **User Interaction** - The om1-avatar frontend displays robot avatars, allowing you to interact with your robot in real time via the BrainPack screen. It is designed to work in conjunction with OM1 robotics backend systems and hardware components.

5. **RTSP Streaming** - The om1-video-processor component streams video and audio data to an RTSP server, enabling real-time video and audio streaming to external systems.

6. **Openmind Privacy System - Face detection and Anonymisation** - A real-time, on-device face detection and blurring module designed to protect personal identity during video capture and streaming. It runs entirely on the robot’s edge device, requiring no cloud or network connectivity.

7. **Auto charging** - The system uses AprilTag-based visual docking combined with Nav2 navigation. When the robot needs to charge, it navigates to the charging station's general area using the Nav2 stack. Once nearby, it switches to precision docking mode where it detects AprilTags mounted on or near the charging dock using its onboard cameras.

    The docking process involves:
        - Visual servoing - The robot aligns itself with the charging station by tracking the AprilTag's pose
        - Precise approach - It approaches the dock using the tag's position and orientation data
        - Physical docking - The robot positions itself so its charging contacts align with the charging pad's contact points

## Getting Started

### Prerequisites
- Docker and docker compose installed (Makes sure the Docker Compose version is v2.32.4)
- Access to all required repositories
- Proper hardware setup

### Quick Start
1. Clone all required repositories:
   ```bash
   git clone https://github.com/OpenMind/OM1.git
   git clone https://github.com/OpenMind/unitree-sdk.git
   git clone https://github.com/OpenMind/OM1-avatar.git
   git clone https://github.com/OpenMind/OM1-video-processor.git
   ```

2. Start the core services:
   ```bash
   # Start OM1
   cd OM1
   docker-compose up om1 -d --no-build

   # Start ROS2 SDK components
   cd ../unitree_sdk
   docker-compose up orchestrator om1_sensor watchdog zenoh_bridge -d --no-build

   # Start Avatar frontend
   cd ../OM1-avatar
   docker-compose up -d --no-build

   # Start Video Processor
   cd ../OM1-video-processor
   docker-compose up -d --no-build
   ```

## What Happens Next

- The BrainPack screen will launch the OM1-avatar frontend, providing a live interface for robot status and control.
- The robot will begin autonomous operation, navigating, mapping, and learning from its environment.
- The om1-video-processor streams video and audio data to an RTSP server, enabling real-time video and audio processing.
- You can now interact with your robot through the user interface.

---

Your robot is now ready to accompany you, assist with tasks, explore new environments, and learn alongside you.
